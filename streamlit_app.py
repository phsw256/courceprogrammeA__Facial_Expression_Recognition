'''
kl:è¿™ä¸ªæ–‡ä»¶ä»å¤´åˆ°å°¾éƒ½æ˜¯æˆ‘å’Œinline chatåˆä½œå®Œæˆçš„ï¼Œè™½ç„¶åŒ…æ˜¯å¤åˆ¶çš„lab5çš„ï¼Œä½†å†™ä¸€ä¸ªmainæ–‡ä»¶è¿˜æ˜¯å¾ˆä¸å®¹æ˜“çš„
è¦å®ç°FER2013çš„è¯†åˆ«ï¼Œæ¨¡å‹å’Œå¤´æ–‡ä»¶å†™å¥½åœ¨modelæ–‡ä»¶å¤¹ï¼Œç°åœ¨ä¸»è¦ç¨‹åºè¦å¹²ä»€ä¹ˆå‘¢
å¹¶éåªè¦å†™mainæ–‡ä»¶
'''

import torch
import torch.nn as nn
import torch.optim as optim
import os
import streamlit as st
import datetime

st.title("courceprogrammeA_Facial_Expression_Recognition")
st.write('#### kl\'s coruce programme.')
st.wrtie('If u use your phone or browser not big enough, click the > above to choose what u want to do with this programme.')
st.write('Unless locally installed, you can't train the models.')

import torchvision
from PIL import Image
import time

from matplotlib import pyplot as plt
from networkx.drawing import draw_planar
from torch.nn.functional import dropout

from models import SimpleMLP, DeepMLP, ResidualMLP, SimpleCNN, MediumCNN, VGGStyleNet, SimpleResNet
from utils import (
    load_FER2013,
    set_seed,
    train_model,
    evaluate_model,
    plot_training_history,
    visualize_model_predictions,
    visualize_conv_filters,
    model_complexity,
    predict
)

save_directory = './ck'
mode=st.sidebar.radio('è¦åšä»€ä¹ˆ',('Train', 'Just_Test', 'Use'))
recorder=[]

if mode=="Train":

    model_type = st.selectbox(
        label = 'é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹',
        options = ('simple_mlp', 'deep_mlp', 'residual_mlp', 'simple_cnn', 'medium_cnn', 'vgg_style', 'resnet'),
        index = 0,
        format_func = str,
        help = 'è¿™é‡Œæœ‰ä¸ªé—®å·ä½ å¯ä»¥ç‚¹å‡»'
    )

    epochs = st.slider(
        label='epochs',
        min_value=0,
        max_value=100,
        value=20,
        step=1,
        help="å…¶å®ä¸ç”¨ç‚¹å‡»ï¼Œé¼ æ ‡ç§»ä¸Šå»å°±è¡Œ"
    )

    batch_size = st.slider(
        label='batch_size',
        min_value=1,
        max_value=512,
        value=128,
        step=1,
        help="ä½ å¯ä»¥è¯•è¯•512"
    )

    learning_rate = st.number_input(
        label='learning_rate',
        min_value=0.0001,
        max_value=0.1,
        value=0.001,
        step=0.0001,
        format="%.4f",
        help="æ­¥å­è¿ˆå¤§äº†ï¼Œå®¹æ˜“å‡ºäº‹"
    )


    use_data_augmentation = st.checkbox(
        label='æ˜¯å¦é‡‡ç”¨æ•°æ®å¢å¼º',
        value=True,
        help='''use_data_augmentation = st.checkbox(
        label='æ˜¯å¦é‡‡ç”¨æ•°æ®å¢å¼º',
        value=True,
        help=""
    )'''
    )

    visualize_filters = st.checkbox(
        label='æ˜¯å¦å¯è§†åŒ–å·ç§¯æ ¸',
        value=True,
        help="ä»…å¯¹CNNæœ‰æ•ˆ"
    )

    visualize_predictions = st.checkbox(
        label='æ˜¯å¦å¯è§†åŒ–é¢„æµ‹ç»“æœ',
        value=True,
        help="è¿™é‡Œä¸çŸ¥é“å¡«ä»€ä¹ˆäº†"
    )

    set_seed(st.number_input(
        label='éšæœºç§å­',
        min_value=0,
        max_value=100,
        value=42,
        step=1,
        help="42æ˜¯å¯¹çš„"
    ))

    in_channels = None
    if model_type == 'resnet':
        in_channels = st.slider(label='in_channels', value=16, min_value=1, max_value=128, step=1)

    if model_type == 'deep_mlp' or model_type == 'residual_mlp':
        dropout_rate = st.slider(label='dropout_rate', value=0.5, min_value=0.0, max_value=1.0, step=0.01)

    if model_type == 'medium_cnn':
        o1 = st.slider(label='out_channel1', value=32, min_value=8, max_value=128, step=1)

    if model_type == 'vgg_style':
        o2 = st.slider(label='out_channel1', value=64, min_value=8, max_value=256, step=1)

    if model_type == 'residual_mlp':
        activation = st.selectbox(
            label='æ¿€æ´»å‡½æ•°',
            options=['relu', 'leaky_relu', 'gelu', 'swish'],
            index=0,
            format_func=str,
        )

    if st.button("Go!"):
        # klæƒ³æŠŠè¶…å‚æ•°è®°ä¸‹æ¥
        recorder = [("batchsize", batch_size), ("epochs", epochs), ("learningrate", learning_rate)]

        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        st.success(f"ä½¿ç”¨è®¾å¤‡: {device}")

        train_loader, valid_loader, test_loader, classes = load_FER2013(
            use_augmentation=use_data_augmentation,
            batch_size=batch_size
        )

        if model_type == 'simple_mlp':
            model = SimpleMLP()
            model_name = "SimpleMLP"
        elif model_type == 'deep_mlp':
            model = DeepMLP(dropout_rate=dropout_rate, use_bn=True, use_dropout=True)
            recorder.append(("dropout_rate", dropout_rate))
            model_name = "DeepMLP"
        elif model_type == 'residual_mlp':
            model = ResidualMLP(dropout_rate=dropout_rate, activation=activation)
            recorder.append(("dropout_rate", dropout_rate))
            recorder.append(("activation", activation))
            model_name = "ResidualMLP"
        elif model_type == 'simple_cnn':
            model = SimpleCNN()
            model_name = "SimpleCNN"
        elif model_type == 'medium_cnn':
            model = MediumCNN(out_channel1= o1,use_bn=True)
            recorder.append(("out_channel1", o1))
            model_name = "MediumCNN"
        elif model_type == 'vgg_style':
            model = VGGStyleNet(out_channel1= o2)
            recorder.append(("out_channel1", o2))
            model_name = "VGGStyleNet"
        elif model_type == 'resnet':
            model = SimpleResNet(num_blocks=[2, 2, 2],in_channels=in_channels)
            recorder.append(("in_channels", in_channels))
            model_name = "SimpleResNet"
        else:
            st.error("æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶åã€‚")
            st.stop()

        print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
        st.success(f"ä½¿ç”¨æ¨¡å‹: {model_name}")



        # è®¡ç®—æ¨¡å‹å¤æ‚åº¦
        print("\nåˆ†ææ¨¡å‹å¤æ‚åº¦:")
        model_complexity(model, device=device)

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        # å¯ä»¥æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

        # ç¡®ä¿checkpointsç›®å½•å­˜åœ¨
        os.makedirs(save_directory, exist_ok=True)


        # è®­ç»ƒæ¨¡å‹
        trained_model, history = train_model(
            model, train_loader, valid_loader, criterion, optimizer, scheduler,
            num_epochs=epochs, device=device, save_dir=save_directory ,recorder=recorder
        )

        # ç»˜åˆ¶è®­ç»ƒå†å²
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_training_history(history, title=f"{timestamp} {model_name} Training History")

        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
        print("\nåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹:")
        test_loss, test_acc = evaluate_model(trained_model, test_loader, criterion, device, classes ,timestamp)

        st.success(f"{model_name} æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")


        # å¦‚æœæ˜¯CNNæ¨¡å‹å¹¶ä¸”éœ€è¦å¯è§†åŒ–å·ç§¯æ ¸
        if visualize_filters and model_type in ['simple_cnn', 'medium_cnn', 'vgg_style', 'resnet']:
            st.write("\nå¯è§†åŒ–å·ç§¯æ ¸:")
            if model_type == 'simple_cnn':
                visualize_conv_filters(trained_model, 'conv1',title=f"{timestamp} {model_name} SimpleCNN Conv1 Filters")
            elif model_type == 'medium_cnn':
                visualize_conv_filters(trained_model, 'conv1', title=f"{timestamp} {model_name} MediumCNN Conv1 Filters")
            elif model_type == 'vgg_style':
                visualize_conv_filters(trained_model, 'features.0', title=f"{timestamp} {model_name} VGGStyleNet Conv1 Filters")
            else:  # resnet
                visualize_conv_filters(trained_model, 'conv1', title=f"{timestamp} {model_name} ResNet Conv1 Filters")

        # å¦‚æœéœ€è¦å¯è§†åŒ–æ¨¡å‹é¢„æµ‹
        if visualize_predictions:
            st.write("\nå¯è§†åŒ–æ¨¡å‹é¢„æµ‹:")
            visualize_model_predictions(trained_model, test_loader, classes, device,title=f"{timestamp} {model_name} Predictions")

        print(f"\n{model_name}çš„è®­ç»ƒå’Œè¯„ä¼°å·²å®Œæˆï¼")
        st.success(f"\n{model_name}çš„è®­ç»ƒå’Œè¯„ä¼°å·²å®Œæˆï¼")


# è·å– ck æ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹æ–‡ä»¶
model_files = [f for f in os.listdir(save_directory) if f.endswith('.pth')]
# åˆ›å»ºä¸‹æ‹‰åˆ—è¡¨ä¾›ç”¨æˆ·é€‰æ‹©æ¨¡å‹
selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹æ–‡ä»¶", model_files)

if mode=="Just_Test":

    if not model_files:
        st.error("ck æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ¨¡å‹æ–‡ä»¶ï¼")
    else:
        st.success(f"å·²é€‰æ‹©æ¨¡å‹æ–‡ä»¶: {selected_model}")

        use_data_augmentation = st.checkbox(
            label='æ˜¯å¦é‡‡ç”¨æ•°æ®å¢å¼º',
            value=True,
            help='''use_data_augmentation = st.checkbox(
                label='æ˜¯å¦é‡‡ç”¨æ•°æ®å¢å¼º',
                value=True,
                help=""
            )'''
        )

        visualize_filters = st.checkbox(
            label='æ˜¯å¦å¯è§†åŒ–å·ç§¯æ ¸',
            value=True,
            help="ä»…å¯¹CNNæœ‰æ•ˆ"
        )

        visualize_predictions = st.checkbox(
            label='æ˜¯å¦å¯è§†åŒ–é¢„æµ‹ç»“æœ',
            value=True,
            help="è¿™é‡Œä¸çŸ¥é“å¡«ä»€ä¹ˆäº†"
        )

        batch_size = st.slider(
            label='batch_size',
            min_value=1,
            max_value=512,
            value=128,
            step=1,
            help="ä½ å¯ä»¥è¯•è¯•512"
        )

        selected_model_name=(selected_model.replace("_best.pth", "")).lower()
        st.success(selected_model_name)

        # åŠ è½½æ¨¡å‹
        model_path = os.path.join(save_directory, selected_model)
        # å®ä¾‹åŒ–æ¨¡å‹å¯¹è±¡
        if selected_model_name == 'simplemlp':
            model = SimpleMLP()
        elif selected_model_name == 'deepmlp':
            model = DeepMLP(dropout_rate=0.5, use_bn=True, use_dropout=True)
        elif selected_model_name == 'residualmlp':
            model = ResidualMLP(activation='relu')
        elif selected_model_name == 'simplecnn':
            model = SimpleCNN()
        elif selected_model_name == 'mediumcnn':
            model = MediumCNN(use_bn=True)
        elif selected_model_name == 'vggstylenet':
            model = VGGStyleNet()
        elif selected_model_name == 'simpleresnet':  # resnet
            model = SimpleResNet(num_blocks=[2, 2, 2])
        else:
            st.error("æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶åã€‚")
            st.stop()

        # åŠ è½½æ¨¡å‹å‚æ•°
        model.load_state_dict(torch.load(model_path))

        st.success(f"æ¨¡å‹ {selected_model} å·²åŠ è½½æˆåŠŸï¼")

        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        st.success(f"ä½¿ç”¨è®¾å¤‡: {device}")

    if st.button("Go!"):
        train_loader, valid_loader, test_loader, classes = load_FER2013(
            use_augmentation=use_data_augmentation,
            batch_size=batch_size
        )

        criterion = nn.CrossEntropyLoss()
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

        print("\nåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹:")
        test_loss, test_acc = evaluate_model(model, test_loader, criterion, device, classes, timestamp)

        st.success(f"{selected_model} æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")

        # å¦‚æœæ˜¯CNNæ¨¡å‹å¹¶ä¸”éœ€è¦å¯è§†åŒ–å·ç§¯æ ¸
        if visualize_filters and selected_model_name in ['simple_cnn', 'medium_cnn', 'vgg_style', 'resnet']:
            print("\nå¯è§†åŒ–å·ç§¯æ ¸:")
            if selected_model_name == 'simple_cnn':
                visualize_conv_filters(model, 'conv1', title=f"{timestamp} {selected_model} SimpleCNN Conv1 Filters")
            elif selected_model_name == 'medium_cnn':
                visualize_conv_filters(model, 'conv1', title=f"{timestamp} {selected_model} MediumCNN Conv1 Filters")
            elif selected_model_name == 'vgg_style':
                visualize_conv_filters(model, 'features.0',
                                       title=f"{timestamp} {selected_model} VGGStyleNet Conv1 Filters")
            else:  # resnet
                visualize_conv_filters(model, 'conv1', title=f"{timestamp} {selected_model} ResNet Conv1 Filters")

        # å¦‚æœéœ€è¦å¯è§†åŒ–æ¨¡å‹é¢„æµ‹
        if visualize_predictions:
            print("\nå¯è§†åŒ–æ¨¡å‹é¢„æµ‹:")
            visualize_model_predictions(model, test_loader, classes, device,
                                        title=f"{timestamp} {selected_model} Predictions")

        print(f"\n{selected_model}çš„è¯„ä¼°å·²å®Œæˆï¼")
        st.success(f"\n{selected_model}çš„è¯„ä¼°å·²å®Œæˆï¼")


if mode=="Use":

    if not model_files:
        st.error("ck æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ¨¡å‹æ–‡ä»¶ï¼")
    else:
        st.success(f"å·²é€‰æ‹©æ¨¡å‹æ–‡ä»¶: {selected_model}")

        selected_model_name = (selected_model.replace("_best.pth", "")).lower()
        st.success(selected_model_name)

        # åŠ è½½æ¨¡å‹
        model_path = os.path.join(save_directory, selected_model)
        # å®ä¾‹åŒ–æ¨¡å‹å¯¹è±¡
        if selected_model_name == 'simplemlp':
            model = SimpleMLP()
        elif selected_model_name == 'deepmlp':
            model = DeepMLP(dropout_rate=0.5, use_bn=True, use_dropout=True)
        elif selected_model_name == 'residualmlp':
            model = ResidualMLP(activation='relu')
        elif selected_model_name == 'simplecnn':
            model = SimpleCNN()
        elif selected_model_name == 'mediumcnn':
            model = MediumCNN(use_bn=True)
        elif selected_model_name == 'vggstylenet':
            model = VGGStyleNet()
        elif selected_model_name == 'simpleresnet':
            model = SimpleResNet(num_blocks=[2, 2, 2])
        else:
            st.error("æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶åã€‚")
            st.stop()

        # åŠ è½½æ¨¡å‹å‚æ•°
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))

        st.success(f"æ¨¡å‹ {selected_model} å·²åŠ è½½æˆåŠŸï¼")

        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        st.success(f"ä½¿ç”¨è®¾å¤‡: {device}")

        img_source = st.sidebar.radio('Please select the source of the facial expression image.',
                                      ('Upload the image', 'Capture the image', 'Select a sample image', 'Clipboard'))

        if img_source == 'Upload the image':
            img_file = st.sidebar.file_uploader('Please upload the facial expression image.',
                                                type=['jpg', 'png', 'jpeg'])
            if img_file is None:
                st.write('#### â† You can select how to upload the image from the sidebar.')

        elif img_source == 'Capture the image':
            img_file = st.sidebar.camera_input('Please capture the facial expression image.')
            if img_file is None:
                st.write('#### â† You can select how to upload the image from the sidebar.')
        elif img_source == 'Select a sample image':
            img_file = st.sidebar.radio(
                'Please select a sample image.',
                ('Sample 1', 'Sample 2', 'Sample 3', 'Sample 4', 'Sample 5', 'Sample 6')
            )

            if img_file == 'Sample 1':
                img_file = 'image/sample1.jpg'
            elif img_file == 'Sample 2':
                img_file = 'image/sample2.jpg'
            elif img_file == 'Sample 3':
                img_file = 'image/sample3.jpg'
            elif img_file == 'Sample 4':
                img_file = 'image/sample4.jpg'
            elif img_file == 'Sample 5':
                img_file = 'image/sample5.jpg'
            elif img_file == 'Sample 6':
                img_file = 'image/sample6.jpg'
            else:
                img_file = None
        else:#img_source == 'Clipboard':
            from PIL import ImageGrab
            img_file = None
            try:
                img = ImageGrab.grabclipboard()
                if img is None:
                    st.error("å‰ªè´´æ¿ä¸­æ²¡æœ‰å›¾ç‰‡ï¼")
                else:
                    img_file = img.convert('RGB')
                    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
                    os.makedirs('image', exist_ok=True)  # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
                    img_file.save(f'image/clipboard_image_{timestamp}.png')
                    img_file = f'image/clipboard_image_{timestamp}.png'
            except Exception as e:
                st.error(f"æ— æ³•ä»å‰ªè´´æ¿è¯»å–å›¾ç‰‡: {e}")

        if img_file is not None:
            if st.button("Go!"):
                with st.spinner('loadingãƒ»ãƒ»ãƒ»'):
                    start_time = time.time()
                    img = Image.open(img_file)

                    transform = torchvision.transforms.Compose([
                        torchvision.transforms.Grayscale(num_output_channels=3),
                        torchvision.transforms.Resize((48, 48)),
                        torchvision.transforms.ToTensor(),
                        torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
                    ])

                    transimg_tensor = transform(img)
                    transimg_np = transimg_tensor.permute(1, 2, 0).numpy()

                    st.image(img, caption='Facial expression image', use_column_width=True)

                    results = predict(transimg_tensor, model, device=device)
                    st.success(f'Elapsed time: {time.time() - start_time:.3f} [sec]')
                    st.subheader('Probs of each emoji:')

                    # Display bar chart
                    st.bar_chart(data=results)

                    # Display emotion
                    emotion = results.idxmax()[0]
                    st.subheader('Predicted emoji:')

                    # Display Big Emoji
                    st.markdown(f'<h1 style="text-align:center;">{emotion}</h1>', unsafe_allow_html=True)

                    # ç”ŸæˆPNGå›¾ç‰‡æŠ¥å‘Š
                    report_path = f"./reportfig/report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{selected_model}.png"
                    plt.figure(figsize=(8, 24))
                    plt.suptitle(f"Expression Analysis Report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}", fontsize=16)
                    plt.subplot(3, 1, 1)
                    plt.title("input image")
                    plt.imshow(img)
                    plt.axis('off')
                    plt.subplot(3, 1, 2)
                    plt.title("transformed image")
                    plt.imshow((transimg_np * 0.5 + 0.5))
                    plt.axis('off')
                    plt.subplot(3, 1, 3)
                    plt.title(f'Probabilities__model:{selected_model}')
                    plt.bar(results.index, results['Probability'], tick_label=results.index)
                    plt.xlabel("Emoji")
                    plt.ylabel("Probability")
                    plt.axis('on')
                    os.makedirs('./reportfig', exist_ok=True)  # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
                    plt.savefig(report_path)
                    st.pyplot(plt.gcf())  # æ˜¾ç¤ºå›¾åƒ
                    plt.close()

                    st.success(f"æŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜åœ¨ {report_path}")



        st.sidebar.divider()

        st.sidebar.caption('This app is not powered by Hugging Face ğŸ¤— .  \n \
                                The ViT model was not fine-tuned on [FER2013 dataset](https://paperswithcode.com/dataset/fer2013).\n \
                                But we have lab5.')
